---
layout: default
title:  "Evaluating Constrained Generation"
date:   2024-04-02 10:48:12 -0700

categories: genai
---

# Probabilistic Text Generation

Today's class of language models are _autoregressive_. They generate one token at a time, and at each generation step, they sample a token from a distribution that depends on all previous tokens.

Given some context $I$, the next token $x_0$ is sampled from the distribution $p(x_0\mid I)$. The next token $x_1$ then depends on $I$ and $x_0$, so it comes from $p(x_1 \mid I, x_0)$.

Then the probability that we sample a sequence $x_0, \ldots, x_\tau$ is given by the product:

$$
\begin{align}
p(x_0, \ldots, x_\tau \mid I) 
&= \prod_{t=0}^\tau p(x_t | x_{0:t-1}, I) \\
&= p(x_0|I)p(x_1|x_0, I)\cdots p(x_\tau | x_0, \ldots, x_{t-1}, I)
\end{align}
$$

# Constrained Text Generation

Given a grammar $G$, let the set of all strings in that grammar be given by $T(G)$ (TODO: look to the ancients and steal their notation).

To denote that a string $t$ is a prefix of another string $s$, we will write $ t \prec s$. 

Blah blah TODO: notation.


During generation, we can't just sample from our original distribution $p$. We have to satisfy some _constraints_. The constraint set at time $t$ will depend on all of the previous tokens, but we will exclude $I$. We will write it
$$
C(x_0, \ldots, x_{t-1})\,.
$$

Wherever it's not ambiguous, we will use the shorthand,
$$
C_t \equiv C(x_0, \ldots, x_{t-1})\,.
$$

Great, now our sampling procedure will be to sample token-wise from our original distribution $p$ _but only accept tokens that are in $C$_.

The distribution will then be

$$
p^C(x_t|x_{0:t-1}, I) \equiv \frac{1}{Z^C_t}\cdot\begin{cases}
p(x_t|x_{0:t-1}, I) & x_t \in C_t \\
0 & else
\end{cases}\,.
$$

For some lazy shorthand, we will restrict the domain of $p^C(x_{0:t-1}, I)$ to $C_t$ so we can simply write

$$
p^C(x_t|x_{0:t-1}, I) = \frac{1}{Z^C_t} p(x_t|x_{0:t-1}, I)\,.
$$

Now, what is this $Z^C_t$ term? Because we have restricted the domain of $p^C$, we need to _renormalize_ it. To rephrase, we need to make sure that our probabilities sum up to $1$, and we need to divide by a number $Z^C_t$ to make this happen. This number is given by

$$
Z^C_t \equiv \sum_{x_t \in C_t} p(x_t|x_{0:t-1}, I)\,.
$$

# Quantifying the "work" done by the constraints

If we have a string generated by $p^C$, it is natural to ask how likely it would have been for our original distribution $p$ to have generated the same string.

If the original model is never "in conflict" with the constraints, we should expect this probability to be relatively high. 

For example, if we have a good prompt that explains that the model should conform to some specific JSON schema, constraining the model's output to that schema shouldn't push us too far "out of distribution" (TODO: explain! blind alleys! etc.). On the other hand, if we say nothing about JSON in the prompt, constraints may "fight" the model and lead to low-probability outputs (things the model never would have said on its own).

For some string $x_0\cdots x_\tau$, in the grammar, this is just 

$$
p(x_0, \ldots, x_\tau \mid I).
$$

**Aside**:
If we wanted to put all $\tau$-length sequences into a lookup table and wanted to spend as few bits as possible on the keys of this table, we would want to use fewer bits to encode the most common strings and more bits to encode the less common strings. The _optimal_ encoding for a string $x_0\cdots x_\tau$ from this standpoint will use $-\log_2p(x_0, \ldots, x_\tau \mid I)$ bits. See [Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding).

Then the _average number of bits_ we spend on a string from $p^C$ is given by (todo: explain notation)
$$
-\left\langle\log_2p(x_0, \ldots, x_\tau \mid I)\right\rangle_{p^C}\,.
$$

Ok, great. Now we have a number that characterizes how "expensive" our constrained strings are. (Psst.... this is the [cross entropy](https://en.wikipedia.org/wiki/Cross-entropy).)

But expensive relative to what?

If we used an encoding scheme based on $p^C$ itself rather than $p$, then the cost would have been lower. Why? The optimal encoding scheme for $p^C$ is optimal precisely because it's the cheapest possible option. That would be given by:

$$
-\left\langle\log_2p^C(x_0, \ldots, x_\tau \mid I)\right\rangle_{p^C}\,.
$$

(Psst... this is the [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)).)

Now we can answer the question "expensive relative to what" by using just the extra cost incurred using the $p$ scheme instead of $p^C$:

$$
-\left\langle\log_2p(x_0, \ldots, x_\tau \mid I)\right\rangle_{p^C} - (-\left\langle\log_2p^C(x_0, \ldots, x_\tau \mid I)\right\rangle_{p^C})
$$

$$
= \left\langle\log_2\frac{p^C(x_0, \ldots, x_\tau \mid I)}{p(x_0, \ldots, x_\tau \mid I)}\right\rangle_{p^C}
$$

We know this is a non-negative quantity because of our reasoning about "optimal" costs earlier. (Psst... it's the [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).)

Expanding the insides of the average a bit:

$$
\frac{p^C(x_0, \ldots, x_\tau \mid I)}{p(x_0, \ldots, x_\tau \mid I)}
= \frac{\prod_{t=0}^\tau \frac{1}{Z^C_t}p(x_t | x_{0:t-1}, I)}{\prod_{t=0}^\tau p(x_t | x_{0:t-1}, I)} = \prod_{t=0}^\tau\frac{1}{Z^C_t}
$$

and putting back inside the average and log:

$$
\left\langle\log_2\frac{p^C(x_0, \ldots, x_\tau \mid I)}{p(x_0, \ldots, x_\tau \mid I)}\right\rangle_{p^C} =  \left\langle\log_2\prod_{t=0}^\tau\frac{1}{Z^C_t}\right\rangle_{p^C}
$$

remembering our logarithm rules...

$$
-\sum_{t=0}^\tau \left\langle \log_2 Z^C_t \right\rangle_{p^C}
$$

What was $Z^C_t$ again?

$$
Z^C_t = \sum_{x_t \in C_t} p(x_t|x_{0:t-1}, I)\,.
$$

$Z^C_t$ is the probability that the original model would spontaneously generate a token from the constraint set at time $t$ (conditioned on the entire history).


### Some TODOs
- high temperature: this just tells us what fraction of all strings fit our grammar
- "forced" grammars recover $p(x_0\cdots x_t)$
- `r".*"` gives value of zero
- be more intentional about notation, e.g. $x_0\cdots x_t$ for concatenation?
- divide by $\tau$ to get a bit-per-token measure

# Calibration
TODO

```python
"Who is that guy from Star Wars? " + select(["Luke Wilson", "Ben Kenobi"])
```